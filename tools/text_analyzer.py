# ğŸ“ tools/text_analyzer.py
from pydantic_ai import RunContext
from models.context import ConversationContext
from typing import Annotated
import re
from collections import Counter

def tool_metadata():
    return {
        "name": "text_analyzer",
        "description": "Analisador avanÃ§ado de textos",
        "version": "1.5.0",
        "author": "NLP Team", 
        "category": "text"
    }

async def analyze_text(
    ctx: RunContext[ConversationContext],
    text: Annotated[str, "Texto para analisar"],
    analysis_type: Annotated[str, "Tipo: 'basic', 'detailed', 'sentiment', 'keywords'"] = "basic"
) -> str:
    """Analisa texto com diferentes nÃ­veis de profundidade"""
    
    if not text.strip():
        return "âŒ ForneÃ§a um texto para analisar"
    
    if analysis_type == 'basic':
        return basic_analysis(text)
    elif analysis_type == 'detailed':
        return detailed_analysis(text)
    elif analysis_type == 'sentiment':
        return sentiment_analysis(text)
    elif analysis_type == 'keywords':
        return keyword_analysis(text)
    else:
        return "âŒ Tipo de anÃ¡lise invÃ¡lido. Use: basic, detailed, sentiment, keywords"

def basic_analysis(text: str) -> str:
    """AnÃ¡lise bÃ¡sica do texto"""
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    paragraphs = text.split('\n\n')
    
    chars_no_spaces = len(text.replace(' ', ''))
    
    return (
        f"ğŸ“ ANÃLISE BÃSICA:\n"
        f"   ğŸ“Š Caracteres: {len(text)}\n"
        f"   ğŸ”¤ Caracteres (sem espaÃ§os): {chars_no_spaces}\n"  
        f"   ğŸ’¬ Palavras: {len(words)}\n"
        f"   ğŸ“„ Frases: {len([s for s in sentences if s.strip()])}\n"
        f"   ğŸ“‹ ParÃ¡grafos: {len([p for p in paragraphs if p.strip()])}\n"
        f"   â±ï¸ Tempo de leitura: ~{max(1, len(words) // 200)} min"
    )

def detailed_analysis(text: str) -> str:
    """AnÃ¡lise detalhada do texto"""
    words = text.split()
    
    # Contagem de tipos de caracteres
    letters = sum(1 for c in text if c.isalpha())
    numbers = sum(1 for c in text if c.isdigit()) 
    spaces = sum(1 for c in text if c.isspace())
    punctuation = sum(1 for c in text if c in '.,!?;:"()[]{}')
    
    # AnÃ¡lise de palavras
    word_lengths = [len(w.strip('.,!?;:"()[]{}')) for w in words]
    avg_word_length = sum(word_lengths) / len(word_lengths) if word_lengths else 0
    
    # Palavras mais comuns
    word_freq = Counter(word.lower().strip('.,!?;:"()[]{}') for word in words)
    most_common = word_freq.most_common(5)
    
    return (
        f"ğŸ“Š ANÃLISE DETALHADA:\n"
        f"   ğŸ”¤ Letras: {letters}\n"
        f"   ğŸ”¢ NÃºmeros: {numbers}\n"
        f"   â­ EspaÃ§os: {spaces}\n"
        f"   ğŸ“ PontuaÃ§Ã£o: {punctuation}\n"
        f"   ğŸ’¬ Total de palavras: {len(words)}\n"
        f"   ğŸ“ MÃ©dia de caracteres por palavra: {avg_word_length:.1f}\n"
        f"   ğŸ“‹ Palavra mais longa: {max(word_lengths) if word_lengths else 0} chars\n"
        f"   ğŸ† Palavras mais frequentes: {', '.join([f'{w}({c})' for w, c in most_common[:3]])}"
    )

def sentiment_analysis(text: str) -> str:
    """AnÃ¡lise de sentimento bÃ¡sica"""
    # Palavras positivas e negativas bÃ¡sicas em portuguÃªs
    positive_words = {
        'bom', 'boa', 'Ã³timo', 'Ã³tima', 'excelente', 'maravilhoso', 'fantÃ¡stico', 
        'incrÃ­vel', 'perfeito', 'adorei', 'amei', 'feliz', 'alegre', 'satisfeito',
        'positivo', 'sucesso', 'vitÃ³ria', 'conquistar', 'vencer', 'legal', 'massa'
    }
    
    negative_words = {
        'ruim', 'pÃ©ssimo', 'pÃ©ssima', 'horrÃ­vel', 'terrÃ­vel', 'odiei', 
        'detesto', 'triste', 'chateado', 'frustrado', 'negativo', 'fracasso',
        'derrota', 'perder', 'problema', 'erro', 'difÃ­cil', 'impossÃ­vel'
    }
    
    words = [word.lower().strip('.,!?;:"()[]{}') for word in text.split()]
    
    positive_count = sum(1 for word in words if word in positive_words)
    negative_count = sum(1 for word in words if word in negative_words)
    
    # Calcular sentimento
    if positive_count > negative_count:
        sentiment = "ğŸ˜Š Positivo"
        confidence = min(90, (positive_count / len(words)) * 100 + 60)
    elif negative_count > positive_count:
        sentiment = "ğŸ˜ Negativo"  
        confidence = min(90, (negative_count / len(words)) * 100 + 60)
    else:
        sentiment = "ğŸ˜ Neutro"
        confidence = 50
    
    return (
        f"ğŸ’­ ANÃLISE DE SENTIMENTO:\n"
        f"   ğŸ¯ Sentimento: {sentiment}\n"
        f"   ğŸ“Š ConfianÃ§a: {confidence:.1f}%\n"
        f"   ğŸ˜Š Palavras positivas: {positive_count}\n"
        f"   ğŸ˜ Palavras negativas: {negative_count}\n"
        f"   ğŸ“ Total analisado: {len(words)} palavras"
    )

def keyword_analysis(text: str) -> str:
    """Extrai palavras-chave do texto"""
    # Palavras irrelevantes (stop words) em portuguÃªs
    stop_words = {
        'a', 'o', 'e', 'de', 'do', 'da', 'em', 'um', 'uma', 'com', 'por', 'para',
        'se', 'que', 'nÃ£o', 'na', 'no', 'como', 'mas', 'ou', 'ao', 'atÃ©', 'dos',
        'das', 'seu', 'sua', 'seus', 'suas', 'ele', 'ela', 'eles', 'elas', 'isso',
        'isto', 'aquilo', 'este', 'esta', 'estes', 'estas', 'esse', 'essa', 'esses',
        'essas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'ser', 'estar', 'ter',
        'haver', 'foi', 'sÃ£o', 'estÃ¡', 'tem', 'mais', 'muito', 'bem', 'jÃ¡', 'sÃ³'
    }
    
    # Limpar e filtrar palavras
    words = [word.lower().strip('.,!?;:"()[]{}') for word in text.split()]
    filtered_words = [w for w in words if w and len(w) > 2 and w not in stop_words]
    
    # Contar frequÃªncias
    word_freq = Counter(filtered_words)
    
    # Palavras-chave (mais de 1 ocorrÃªncia)
    keywords = [(word, count) for word, count in word_freq.most_common() if count > 1]
    
    # Palavras Ãºnicas importantes (mais de 4 caracteres)
    unique_important = [word for word, count in word_freq.items() 
                       if count == 1 and len(word) > 4][:10]
    
    result = f"ğŸ” ANÃLISE DE PALAVRAS-CHAVE:\n"
    
    if keywords:
        result += f"   ğŸ·ï¸ Palavras-chave principais:\n"
        for word, count in keywords[:8]:
            result += f"      â€¢ {word} ({count}x)\n"
    
    if unique_important:
        result += f"   âœ¨ Termos Ãºnicos relevantes: {', '.join(unique_important[:5])}\n"
    
    result += f"   ğŸ“Š VocabulÃ¡rio Ãºnico: {len(word_freq)} palavras distintas\n"
    result += f"   ğŸ¯ Densidade de palavras-chave: {len(keywords)}/{len(filtered_words)}"
    
    return result

analyze_text.__tool_metadata__ = tool_metadata()

# ========================
